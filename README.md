# CRS_Recommendation_Classifiaction
This is a project that aims to to classify conversational recommendation based on 8 quality factors with deep learning. The study focuses on Conversational Recommender Systems (CRS) and proposes a method for classifying recommendations as good or bad. Traditional conversational recommendation metrics like BLEU, ROGUE, and METEOR are not sophisticated enough to assess recommendation quality. A shift towards different metrics is needed to assess recommendation quality. Eight quality factors, length, readability, repetition, word importance, polarity, subjectivity, grammar, and feature appearance are proposed to be more relevant, explainable, and impactful metrics to assess conversational recommendation quality. Towards that end, three different neural networks are created using GPT2, GPT-NEO, and t5 as base models that embed a conversational recommendation and factor in the eight aforementioned quality factors as inputs to a linear residual network architecture to classify recommendations. The GPT-NEO model achieves the highest average prediction accuracy at 83%, GPT2 has an average accuracy of 78%, and t5 74%. Individual Conditional Expectation analysis shows that grammar, feature appearance, and repetition are the most impactful quality factors. A Shapley value analysis shows each factor can push model predictions toward bad or good classes for all three models. The 8 quality factors assess recommendation quality more meaningfully, accurately, and contextually than current standard methods.

# Folder Organization
1. The Presentation and thesis paper are in the main repository
2. The dataset and other relevant data files are in Data folder, or the zip folder DataFilesForTesting in the main repository. Most of the code assumes the data files are in the current folder, and so copying the zip file around may be convienant to run and play around with the code.
3. The code to run the neural networks is in the models file. Each of the 3 base models (GPT2, GPT-NEO, and t5) have very similar code, with only minor differences made such as what tokenizer and model are loaded initially, otherwise the files are identical
4. The code that performs the ICE and SHAP analysis is in the ICE_shap folder. The files are nearly identical as with the model codes. The class definitions for the neural networks have been adjusted from the versions in the models folder as the pytorch trainer could not be used for the tasks, so certain aspects of data prepearation like where input data was converted to tensors has been shuffled around. Otherwise, the main changes in these code files are the addition of the ICE/SHAP function, and changes to the main to perform the analysis.
5. The Charts & Figures folder contains the results of the model with different graphs for quick perusal.
6. The ChartMakingcode folder contains the code used to create the figures in the Charts&Figures folder. The accuracy & loss results for each model are in their own labeled folder, charts showing the results of multiple models are in the MultiModelGraphs notebook, and the charts for the ICE and SHAP analysis are in the ICE_SHAP_Charts folder within ChartMakingCode.

# Using the code
To follow the process from start to finish: 
1. Use the process_collate_datasets.py file in the utilityFiles folder. This file reads in the E-redial and INSPIRED data, determines the 8 quality scores for each conversation, and then writes the results to different files. This is how the datasets were combined, and the scores generated. This process takes a while due to the grammar and feature appearance scoring. This can be adjusted by doing less grammar checking, increasing some of the parallelization, and by adjusting the summarization settings with BART for feature appearance.
2. Use the readInAndClasifyData.py file which created the raget labels for the dataset based on the quality factor scores. This needs to be done both for the nonstandardized and standardized datastes. There is also a show historgram function in this file for a quick look at the score distributions of each quality factor.
3. Run the code in the models file using the newly generated combined dataset.
4. Use the findBestModel.py file from the utilityFiles folder to search through the logger directories to find wich model (if you are doing multiple independent runs) has the best accuracy scores.
5. Use the best model to perform the ICE and SHAP analysis with the code in the ICE_shap files. Note, the ICE data explicitly only works for normalized data, and the results mentions in the paper / presentation from the ICE and SHAP analysis were used on standardized data. The SHAP code should work with nonstandardized data, but the ICE code will have to be adjusted if you want to test the results of nonstandardized data.
6. Collate the results into data file, and use the notebooks in the ChartMakingcode to graph and visualize model results. 
